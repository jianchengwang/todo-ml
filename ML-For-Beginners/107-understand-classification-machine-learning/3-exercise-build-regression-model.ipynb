{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import pandas\r\n",
    "#pip install statsmodels\r\n",
    "#wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/graphing.py\r\n",
    "#wget https://raw.githubusercontent.com/MicrosoftDocs/mslearn-introduction-to-machine-learning/main/Data/avalanche.csv\r\n",
    "\r\n",
    "#Import the data from the .csv file\r\n",
    "dataset = pandas.read_csv('avalanche.csv', delimiter=\"\\t\")\r\n",
    "\r\n",
    "#Let's have a look at the data\r\n",
    "dataset"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import graphing # custom graphing code. See our GitHub repo for details\r\n",
    "\r\n",
    "graphing.box_and_whisker(dataset, label_x=\"avalanche\", label_y=\"surface_hoar\", show=True)\r\n",
    "graphing.box_and_whisker(dataset, label_x=\"avalanche\", label_y=\"fresh_thickness\", show=True)\r\n",
    "graphing.box_and_whisker(dataset, label_x=\"avalanche\", label_y=\"weak_layers\", show=True)\r\n",
    "graphing.box_and_whisker(dataset, label_x=\"avalanche\", label_y=\"no_visitors\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Here we import a function that splits datasets according to a given ratio\r\n",
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "# Split the dataset in an 70/30 train/test ratio. \r\n",
    "train, test = train_test_split(dataset, test_size=0.3, random_state=2)\r\n",
    "print(train.shape)\r\n",
    "print(test.shape)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import statsmodels.formula.api as smf\r\n",
    "import graphing # custom graphing code. See our GitHub repo for details\r\n",
    "\r\n",
    "# Perform logistic regression.\r\n",
    "model = smf.logit(\"avalanche ~ weak_layers\", train).fit()\r\n",
    "\r\n",
    "print(\"Model trained\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(model.summary())"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# predict to get a probability\r\n",
    "\r\n",
    "# get first 3 samples from dataset\r\n",
    "samples = test[\"weak_layers\"][:4]\r\n",
    "\r\n",
    "# use the model to get predictions as possibilities\r\n",
    "estimated_probabilities = model.predict(samples)\r\n",
    "\r\n",
    "# Print results for each sample\r\n",
    "for sample, pred in zip(samples,estimated_probabilities):\r\n",
    "    print(f\"A weak_layer with value {sample} yields a {pred * 100:.2f}% chance of an avalanche.\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# plot the model\r\n",
    "# Show a graph of the result\r\n",
    "predict = lambda x: model.predict(pandas.DataFrame({\"weak_layers\": x}))\r\n",
    "\r\n",
    "graphing.line_2D([(\"Model\", predict)],\r\n",
    "                 x_range=[-20,40],\r\n",
    "                 label_x=\"weak_layers\", \r\n",
    "                 label_y=\"estimated probability of an avalanche\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "print(\"Minimum number of weak layers:\", min(train.weak_layers))\r\n",
    "print(\"Maximum number of weak layers:\", max(train.weak_layers))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import numpy as np\r\n",
    "\r\n",
    "# Get actual rates of avalanches at 0 years\r\n",
    "avalanche_outcomes_for_0_layers = train[train.weak_layers == 0].avalanche\r\n",
    "print(\"Average rate of avalanches for 0 weak layers of snow\", np.average(avalanche_outcomes_for_0_layers))\r\n",
    "\r\n",
    "# Get actual rates of avalanches at 10 years\r\n",
    "avalanche_outcomes_for_10_layers = train[train.weak_layers == 10].avalanche\r\n",
    "print(\"Average rate of avalanches for 10 weak layers of snow\", np.average(avalanche_outcomes_for_10_layers))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# threshold to get an absolute value\r\n",
    "threshold = 0.5\r\n",
    "\r\n",
    "# Add classification to the samples we used before\r\n",
    "for sample, pred in list(zip(samples,estimated_probabilities)):\r\n",
    "    print(f\"A weak_layer with value {sample} yields a chance of {pred * 100:.2f}% of an avalanche. Classification = {pred > threshold}\")\r\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Classify the mdel predictions using the threshold\r\n",
    "predictions = model.predict(test) > threshold\r\n",
    "\r\n",
    "# Compare the predictions to the actual outcomes in the dataset\r\n",
    "accuracy = np.average(predictions == test.avalanche)\r\n",
    "\r\n",
    "# Print the evaluation\r\n",
    "print(f\"The model correctly predicted outcomes {accuracy * 100:.2f}% of time.\")"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.3",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.3 32-bit"
  },
  "interpreter": {
   "hash": "586dbcb5c5852d95733b4f6988d368a04938dc62b7d3e4b2ef6fb3176efba2da"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}